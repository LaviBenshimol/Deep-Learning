{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LaviBenshimol/Deep-Learning/blob/master/OAI_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Offensive AI Course Homework\n",
        "* Course website: https://ymirsky.github.io/course/\n",
        "* Course edition: Fall 2024\n",
        "* Lecturer: Dr. Yisroel Mirsky\n",
        "* Due Date: 31/12/24\n",
        "## Instructions\n",
        "* This homework can be done in groups of **two**\n",
        "* Make a copy of this notebook and solve the problems below.\n",
        "* Do not use the ART toolbox in this homework\n",
        "* Orgnization: In each section there are two cells: (1) a cell with a function with some missing code where you must complete the missing code (do not change the existing code), (2) a cell to test your code. Before you submit your work, make sure that the test cell in every section can run your code.\n",
        "* Submission: (1) generate a sharable link to your notebook, (2) submit your link by [clicking here](https://docs.google.com/forms/d/e/1FAIpQLSexY-uV7jTOosgRt863ltfcHTsNO6lWanMXCzQd_uglgddodw/viewform?usp=sf_link)  \n",
        "* Each day past the deadline is -15 pnts\n",
        "\n",
        "\n",
        "## Sections:\n",
        "1. BIM (40 points)\n",
        "2. Random Starts (10 points)\n",
        "3. Masking (10 points)\n",
        "4. Color Limited Attacks (20 points)\n",
        "5. PGD (20 points)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XZaPqx8EjI7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "These cells will setup your environment\n"
      ],
      "metadata": {
        "id": "yd2iqJxoqQA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### Load Modules ####\n",
        "\n",
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "import scipy.linalg\n",
        "\n",
        "## Imports for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython.display import set_matplotlib_formats\n",
        "set_matplotlib_formats('svg', 'pdf') # For export\n",
        "from matplotlib.colors import to_rgb\n",
        "import matplotlib\n",
        "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "## Progress bar\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "## PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "# Torchvision\n",
        "import torchvision\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision import transforms\n",
        "# PyTorch Lightning\n",
        "try:\n",
        "    import pytorch_lightning as pl\n",
        "except ModuleNotFoundError: # Google Colab does not have PyTorch Lightning installed by default. Hence, we do it here if necessary\n",
        "    !pip install --quiet pytorch-lightning>=1.4\n",
        "    import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "# Path to the folder where the datasets are/should be downloaded (e.g. MNIST)\n",
        "DATASET_PATH = \"../data\"\n",
        "\n",
        "# Setting the seed\n",
        "#pl.seed_everything(42)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "#torch.backends.cudnn.determinstic = True\n",
        "#torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Fetching the device that will be used throughout this notebook\n",
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "print(\"Using device\", device)"
      ],
      "metadata": {
        "id": "w0PWkC7NogAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### load dataset ####\n",
        "\n",
        "import urllib.request\n",
        "from urllib.error import HTTPError\n",
        "import zipfile\n",
        "# Github URL where the dataset is stored for this tutorial\n",
        "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial10/\"\n",
        "# Files to download\n",
        "pretrained_files = [(DATASET_PATH, \"TinyImageNet.zip\")]\n",
        "# Create checkpoint path if it doesn't exist yet\n",
        "os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "\n",
        "# For each file, check whether it already exists. If not, try downloading it.\n",
        "for dir_name, file_name in pretrained_files:\n",
        "    file_path = os.path.join(dir_name, file_name)\n",
        "    if not os.path.isfile(file_path):\n",
        "        file_url = base_url + file_name\n",
        "        print(f\"Downloading {file_url}...\")\n",
        "        try:\n",
        "            urllib.request.urlretrieve(file_url, file_path)\n",
        "        except HTTPError as e:\n",
        "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)\n",
        "        if file_name.endswith(\".zip\"):\n",
        "            print(\"Unzipping file...\")\n",
        "            with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(file_path.rsplit(\"/\",1)[0])\n",
        "\n",
        "\n",
        "#### load and setup victim model (resnet34) ####\n",
        "\n",
        "# Load CNN architecture pretrained on ImageNet\n",
        "victim_model = torchvision.models.resnet34(pretrained=True)\n",
        "victim_model = victim_model.to(device)\n",
        "\n",
        "# No gradients needed for the network\n",
        "victim_model.eval()\n",
        "for p in victim_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Mean and Std from ImageNet\n",
        "NORM_MEAN = np.array([0.485, 0.456, 0.406])\n",
        "NORM_STD = np.array([0.229, 0.224, 0.225])\n",
        "# No resizing and center crop necessary as images are already preprocessed.\n",
        "plain_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=NORM_MEAN,\n",
        "                         std=NORM_STD)\n",
        "])\n",
        "\n",
        "# Load dataset and create data loader\n",
        "imagenet_path = os.path.join(DATASET_PATH, \"TinyImageNet/\")\n",
        "assert os.path.isdir(imagenet_path), f\"Could not find the ImageNet dataset at expected path \\\"{imagenet_path}\\\". \"\n",
        "dataset = torchvision.datasets.ImageFolder(root=imagenet_path, transform=plain_transforms)\n",
        "data_loader = data.DataLoader(dataset, batch_size=32, shuffle=True, drop_last=False, num_workers=8)\n",
        "\n",
        "# Load label names to interpret the label numbers 0 to 999\n",
        "with open(os.path.join(imagenet_path, \"label_list.json\"), \"r\") as f:\n",
        "    label_names = json.load(f)\n",
        "\n",
        "def get_label_index(lab_str):\n",
        "    assert lab_str in label_names, f\"Label \\\"{lab_str}\\\" not found. Check the spelling of the class.\"\n",
        "    return label_names.index(lab_str)"
      ],
      "metadata": {
        "id": "tHXzvRXbpGwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### Some helper functions to compute and view results ####\n",
        "\n",
        "def eval_model(victim_model, dataset_loader, img_func=None):\n",
        "    tp, tp_5, counter = 0., 0., 0.\n",
        "    for imgs, labels in tqdm(dataset_loader, desc=\"Validating...\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        if img_func is not None:\n",
        "            imgs = img_func(imgs, labels)\n",
        "        with torch.no_grad():\n",
        "            preds = victim_model(imgs)\n",
        "        tp += (preds.argmax(dim=-1) == labels).sum()\n",
        "        tp_5 += (preds.topk(5, dim=-1)[1] == labels[...,None]).any(dim=-1).sum()\n",
        "        counter += preds.shape[0]\n",
        "    acc = tp.float().item()/counter\n",
        "    top5 = tp_5.float().item()/counter\n",
        "    print(f\"Top-1 error: {(100.0 * (1 - acc)):4.2f}%\")\n",
        "    print(f\"Top-5 error: {(100.0 * (1 - top5)):4.2f}%\")\n",
        "    return acc, top5\n",
        "\n",
        "def show_prediction(img, label, pred, K=5, adv_img=None, noise=None):\n",
        "\n",
        "    if isinstance(img, torch.Tensor):\n",
        "        # Tensor image to numpy\n",
        "        img = img.cpu().permute(1, 2, 0).numpy()\n",
        "        img = (img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        img = np.clip(img, a_min=0.0, a_max=1.0)\n",
        "        label = label.item()\n",
        "\n",
        "    # Plot on the left the image with the true label as title.\n",
        "    # On the right, have a horizontal bar plot with the top k predictions including probabilities\n",
        "    if noise is None or adv_img is None:\n",
        "        fig, ax = plt.subplots(1, 2, figsize=(10,2), gridspec_kw={'width_ratios': [1, 1]})\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(12,2), gridspec_kw={'width_ratios': [1, 1, 1, 1, 2]})\n",
        "\n",
        "    ax[0].imshow(img)\n",
        "    ax[0].set_title(label_names[label])\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    if adv_img is not None and noise is not None:\n",
        "        # Visualize adversarial images\n",
        "        adv_img = adv_img.cpu().permute(1, 2, 0).numpy()\n",
        "        adv_img = (adv_img * NORM_STD[None,None]) + NORM_MEAN[None,None]\n",
        "        adv_img = np.clip(adv_img, a_min=0.0, a_max=1.0)\n",
        "        ax[1].imshow(adv_img)\n",
        "        ax[1].set_title('Adversarial')\n",
        "        ax[1].axis('off')\n",
        "        # Visualize noise\n",
        "        noise = noise.cpu().permute(1, 2, 0).numpy()\n",
        "        noise = noise * 0.5 + 0.5 # Scale between 0 to 1\n",
        "        ax[2].imshow(noise)\n",
        "        ax[2].set_title('Noise')\n",
        "        ax[2].axis('off')\n",
        "        # buffer\n",
        "        ax[3].axis('off')\n",
        "\n",
        "    if abs(pred.sum().item() - 1.0) > 1e-4:\n",
        "        pred = torch.softmax(pred, dim=-1)\n",
        "    topk_vals, topk_idx = pred.topk(K, dim=-1)\n",
        "    topk_vals, topk_idx = topk_vals.cpu().numpy(), topk_idx.cpu().numpy()\n",
        "    ax[-1].barh(np.arange(K), topk_vals*100.0, align='center', color=[\"C0\" if topk_idx[i]!=label else \"C2\" for i in range(K)])\n",
        "    ax[-1].set_yticks(np.arange(K))\n",
        "    ax[-1].set_yticklabels([label_names[c] for c in topk_idx])\n",
        "    ax[-1].invert_yaxis()\n",
        "    ax[-1].set_xlabel('Confidence')\n",
        "    ax[-1].set_title('Predictions')\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "WyFY_FSrqz3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure the victim model is setup correctly"
      ],
      "metadata": {
        "id": "fDPmRcGcrONl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "exmp_batch, label_batch = next(iter(data_loader))\n",
        "with torch.no_grad():\n",
        "    preds = victim_model(exmp_batch.to(device))\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], preds[i])"
      ],
      "metadata": {
        "id": "6Z6kPfRUrMO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. sign-PGD\n",
        "\n",
        "In this section, complete the code below to impliment the iterative sign-PGD algorithm, for an $\\ell_{\\infty}$ bound.\n",
        "\n",
        "Features:\n",
        "* The user should be able to choose if the attack is targeted or not targeted\n",
        "* If ``targeted==True``, then sign-PGD should perform a targeted attack using ``labels`` as the target labels. The labels should be class indexes, e.g., ``labels=tensor([23, 4, 7])``\n",
        "* If ``targeted==False``, then ``labels`` should contain the actual labels for the batch and sign-PGD should perform an untargeted attack.\n",
        "\n",
        "Reminders:\n",
        "* alpha is the gradient step size to be applied to ``adv_images``\n",
        "* eps is the bound of how far ``adv_images`` is allowed to travel from ``images`` (individually)\n",
        "\n"
      ],
      "metadata": {
        "id": "5DGcxrJonm1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def signPGD(model, images, labels, eps=0.3, alpha=0.01, steps=10, targeted=False,  pixelclip=(-2.6,2.6)):\n",
        "\n",
        "    images = images.clone().to(device)\n",
        "    labels = labels.clone().to(device)\n",
        "\n",
        "    loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    ### Change or move code from here on ###\n",
        "\n",
        "    adv_images = images.clone().detach()\n",
        "    adv_images.requires_grad = True\n",
        "    outputs = model(adv_images)\n",
        "\n",
        "    # hints:\n",
        "\n",
        "      # Calculate loss\n",
        "      # cost = loss(...)\n",
        "\n",
        "      # Update adversarial images\n",
        "      # grad = ...\n",
        "\n",
        "      # clipping...\n",
        "          # ...\n",
        "          # the images are zscore normalized so the pixels should not exceed the range in pixelclip\n",
        "\n",
        "    ### Don't change this code:\n",
        "\n",
        "    return adv_images, alpha*grad.sign()  # grad is the gradient (pertubation)"
      ],
      "metadata": {
        "id": "nxR8jPFmjKTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untargeted attacks\n",
        "adv_imgs_ut, noise_grad_ut = signPGD(victim_model, exmp_batch, label_batch, eps=0.3, alpha=.01, steps=10, targeted=False,  pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_ut = victim_model(adv_imgs_ut.to(device))\n",
        "\n",
        "# Targeted attacks\n",
        "targets = (label_batch + 100) % 1000\n",
        "adv_imgs_t, noise_grad_t = signPGD(victim_model, exmp_batch, targets, eps=0.3, alpha=.01, steps=10, targeted=True,  pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_t = victim_model(adv_imgs_t.to(device))\n",
        "\n",
        "print(\"Untargeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_ut[i], adv_img=adv_imgs_ut[i], noise=100*noise_grad_ut[i])\n",
        "print(\"Targeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_t[i], adv_img=adv_imgs_t[i], noise=100*noise_grad_t[i])"
      ],
      "metadata": {
        "id": "MjenUQPAu1WS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Random Starts\n",
        "\n",
        "Now let's add random starts to sign-PGD.\n",
        "A random start is where we start sign-PGD near x in an attempt to find a better solution. This is accomplished by using x + n in the first iteration, where n contains random values and where x + n is within the distance epsilon from x\n",
        "\n",
        "Feature to impliment:\n",
        "* Let the user choose whether the algorithm should use a random start or not\n",
        "\n",
        "Copy your code below to the function ``signPGD2`` and add the option for a random start"
      ],
      "metadata": {
        "id": "Y-wkimvqF1uE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def signPGD2(model, images, labels, eps=0.3, alpha=0.01, steps=10, targeted=False, random_start=True, pixelclip=(-2.6,2.6)):\n",
        "\n",
        "    # ....\n",
        "\n",
        "    return adv_images, alpha*grad.sign()"
      ],
      "metadata": {
        "id": "NFhvwv2RHEls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let try several trials of BIM on the same image\n",
        "preds = []\n",
        "for i in range(40):\n",
        "  adv_imgs, noise_grad = signPGD2(victim_model, exmp_batch[0:1], label_batch[0:1], eps=0.3, alpha=.01, steps=50, targeted=False, random_start=True, pixelclip=(-2.6,2.6))\n",
        "  with torch.no_grad():\n",
        "    pred = victim_model(adv_imgs.to(device))[0,label_batch[0]].cpu().detach().numpy()\n",
        "    preds.append( pred )\n",
        "\n",
        "plt.hist(preds)\n",
        "plt.title(\"Confidence Distribution Among Attacks on same Image. Lower is better\")"
      ],
      "metadata": {
        "id": "Mkh00BUiHTtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Masked Attacks\n",
        "\n",
        "In some cases an attack is limited to a certain region within the image. In this section, upgrade your ``signPGD2`` procedure so that it will only add a pertubation to one half of an image, yet still fool the victim.\n",
        "\n",
        "Features:\n",
        "* Let the user choose whether to add a pertubation to the left half, right half, or entire image.\n",
        "\n",
        "Hint: you can multiply a tensor with a tensor of zeroes and ones to erase (focus) on relevant pixels."
      ],
      "metadata": {
        "id": "qdsFO1GGqP58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def signPGD3(model, images, labels, eps=0.3, alpha=0.01, steps=10, targeted=False, random_start=True, mask=None, pixelclip=(-2.6,2.6)):\n",
        "    if mask is None:\n",
        "      M = torch.ones(images.shape[1:])\n",
        "    if mask == \"left\":\n",
        "      M = torch.zeros(images.shape)\n",
        "      M[:, :, :, :images.shape[3]//2] = 1\n",
        "    if mask == \"right\":\n",
        "      M = torch.zeros(images.shape)\n",
        "      M[:, :, :, images.shape[3]//2:] = 1\n",
        "\n",
        "    #...\n",
        "\n",
        "    return adv_images, alpha*grad.sign()"
      ],
      "metadata": {
        "id": "D15KxoAFxQug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untargeted attacks\n",
        "adv_imgs_ut, noise_grad_ut = signPGD3(victim_model, exmp_batch, label_batch, eps=0.3, alpha=.01, steps=10, targeted=False, mask=\"left\", pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_ut = victim_model(adv_imgs_ut.to(device))\n",
        "\n",
        "# Targeted attacks\n",
        "targets = (label_batch + 100) % 1000\n",
        "adv_imgs_t, noise_grad_t = signPGD3(victim_model, exmp_batch, targets, eps=0.3, alpha=.01, steps=10, targeted=True, mask=\"left\", pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_t = victim_model(adv_imgs_t.to(device))\n",
        "\n",
        "print(\"Untargeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_ut[i], adv_img=adv_imgs_ut[i], noise=100*noise_grad_ut[i])\n",
        "print(\"Targeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_t[i], adv_img=adv_imgs_t[i], noise=100*noise_grad_t[i])"
      ],
      "metadata": {
        "id": "z-hrUpXFxcbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Color Limited Attacks\n",
        "\n",
        "Now we want to change a specific color channel of the image (e.g., attack the victim by making the image more red in certain pixels)\n",
        "\n",
        "Features:\n",
        "* Let the user choose which channels the pertubation should modify: red, green, blue, or all together (default)."
      ],
      "metadata": {
        "id": "dznJ-TpuundC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def signPGD4(model, images, labels, eps=0.3, alpha=0.01, steps=10, targeted=False, random_start=True, mask=None, channel=None, pixelclip=(-2.6,2.6)):\n",
        "    if mask is None:\n",
        "      M = torch.ones(images.shape[1:])\n",
        "    if mask == \"left\":\n",
        "      M = torch.ones(images.shape[1:])\n",
        "      M[:, :, :, :images.shape[3]//2] = 1\n",
        "    if mask == \"right\":\n",
        "      M = torch.ones(images.shape[1:])\n",
        "      M[:, :, :, images.shape[3]//2:] = 1\n",
        "\n",
        "    #...if channel...\n",
        "\n",
        "    return adv_images, alpha*grad.sign()"
      ],
      "metadata": {
        "id": "lcpKL6GFy86E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untargeted attacks\n",
        "adv_imgs_ut, noise_grad_ut = signPGD4(victim_model, exmp_batch, label_batch, eps=0.3, alpha=.01, steps=10, targeted=False, mask=None, channel=\"red\",  pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_ut = victim_model(adv_imgs_ut.to(device))\n",
        "\n",
        "# Targeted attacks\n",
        "targets = (label_batch + 100) % 1000\n",
        "adv_imgs_t, noise_grad_t = signPGD4(victim_model, exmp_batch, targets, eps=0.3, alpha=.01, steps=10, targeted=True, mask=None, channel= \"red\", pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_t = victim_model(adv_imgs_t.to(device))\n",
        "\n",
        "print(\"Untargeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_ut[i], adv_img=adv_imgs_ut[i], noise=100*noise_grad_ut[i])\n",
        "print(\"Targeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_t[i], adv_img=adv_imgs_t[i], noise=100*noise_grad_t[i])"
      ],
      "metadata": {
        "id": "Jm1z4hD4y86W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. PGD $\\ell_2$ (10 points)\n",
        "Update signPGD4 to perform full gradient (not sign) PGD, and:\n",
        "- epsilon bounded to an $\\ell_2$\n",
        "- normalized steps\n",
        "- optimization performed using the ADAM optimizer\n"
      ],
      "metadata": {
        "id": "qW5v-IN-EiUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def PGD_l2(model, images, labels, eps=0.3, alpha=0.01, steps=10, targeted=False, random_start=True, mask=None, channel=None, pixelclip=(-2.6,2.6)):\n",
        "    #...\n",
        "\n",
        "    return adv_images, alpha*grad.sign()"
      ],
      "metadata": {
        "id": "cyoUKQlmu_yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Untargeted attacks\n",
        "adv_imgs_ut, noise_grad_ut = PGD_l2(victim_model, exmp_batch, label_batch, eps=0.3, alpha=.01, steps=10, targeted=False, mask=None, channel=None,  pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_ut = victim_model(adv_imgs_ut.to(device))\n",
        "\n",
        "# Targeted attacks\n",
        "targets = (label_batch + 100) % 1000\n",
        "adv_imgs_t, noise_grad_t = PGD_l2(victim_model, exmp_batch, targets, eps=0.3, alpha=.01, steps=10, targeted=True, mask=None, channel= None, pixelclip=(-2.6,2.6))\n",
        "with torch.no_grad():\n",
        "    adv_preds_t = victim_model(adv_imgs_t.to(device))\n",
        "\n",
        "print(\"Untargeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_ut[i], adv_img=adv_imgs_ut[i], noise=100*noise_grad_ut[i])\n",
        "print(\"Targeted Results\")\n",
        "for i in range(0,5):\n",
        "    show_prediction(exmp_batch[i], label_batch[i], adv_preds_t[i], adv_img=adv_imgs_t[i], noise=100*noise_grad_t[i])"
      ],
      "metadata": {
        "id": "S6OJpsctpt-U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}